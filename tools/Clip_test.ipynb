{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59fb5ce5",
   "metadata": {},
   "source": [
    "# Generate Language CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8791c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5359702a",
   "metadata": {
    "code_folding": [
     0,
     5,
     13
    ]
   },
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.loc[:, ['video','question','answer','qid','type','a0','a1','a2','a3','a4']]\n",
    "    return data\n",
    "\n",
    "def vaild_text_length(token):\n",
    "    i = 0\n",
    "    for idx in token[0]:\n",
    "        if idx == 0:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "def encode_text(model, text):\n",
    "    text_token = clip.tokenize(text).to('cuda')\n",
    "    text_length = vaild_text_length(text_token)\n",
    "    x = model.token_embedding(text_token).type(model.dtype)\n",
    "    x = x + model.positional_embedding.type(model.dtype)\n",
    "    x = x.permute(1,0,2)\n",
    "    x = model.transformer(x)\n",
    "    x = x.permute(1,0,2)\n",
    "    x = model.ln_final(x).type(model.dtype)\n",
    "    # x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "    return x, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a5af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load('ViT-L/14@336px', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b52c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'  #[train, val, test]\n",
    "csv_data = read_csv(f'../dataset/nextqa/{mode}.csv')\n",
    "h5_file = f'../../data/nextqa/qas_bert/clip_ft_{mode}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba41718f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816cde84068545d3a79bd2e409123f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encoding qas:   0%|          | 0/34132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(h5_file):\n",
    "    f = h5py.File(h5_file, 'w')\n",
    "else:\n",
    "    f = h5py.File(h5_file, 'w')\n",
    "    \n",
    "qas_feats = []\n",
    "qas_length = []\n",
    "for idx, data in tqdm(csv_data.iterrows(), desc='encoding qas', total=csv_data.shape[0]):\n",
    "    candidate_qas = []\n",
    "    candidate_qas_length = []\n",
    "    eot_token = '<|endoftext|>'\n",
    "    for i in range(5):\n",
    "        qas = data['question'] + eot_token + data[f'a{str(i)}']\n",
    "        encode_qas, encode_qas_length = encode_text(clip_model, qas)\n",
    "        candidate_qas.append(encode_qas.detach().cpu().numpy())\n",
    "        candidate_qas_length.append(encode_qas_length)\n",
    "        \n",
    "    candidate_qas = np.array(candidate_qas)\n",
    "    candidate_qas_length = np.array(candidate_qas_length)\n",
    "    qas_feats.append(candidate_qas)\n",
    "    qas_length.append(candidate_qas_length)\n",
    "\n",
    "f.create_dataset('feat', data=qas_feats)\n",
    "f.create_dataset('qas_length', data=qas_length)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69caaad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1, 77, 768) [13 13 12 12 13]\n"
     ]
    }
   ],
   "source": [
    "# Check data\n",
    "with h5py.File(h5_file, 'r') as fp:\n",
    "    print(fp['feat'][100].shape, fp['qas_length'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a72b499",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   328, 49407,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.tokenize('I')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231a3ae",
   "metadata": {},
   "source": [
    "# Generate Video CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe4aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import skvideo.io\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4f6381",
   "metadata": {
    "code_folding": [
     0,
     5,
     10,
     15
    ]
   },
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        r = json.load(f)\n",
    "    return r\n",
    "\n",
    "def get_video_path(map_vid, vid):\n",
    "    video_root = \"../../data/raw_data/video\"\n",
    "    path = os.path.join(video_root,map_vid.get(str(vid))+'.mp4')\n",
    "    return path\n",
    "\n",
    "def read_csv(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.loc[:, ['video','question','answer','qid','type','a0','a1','a2','a3','a4']]\n",
    "    return data\n",
    "\n",
    "def extract_clips_with_consecutive_frames(path, num_clips, num_frames_per_clip, preprocess, clip_model):\n",
    "    \"\"\"\n",
    "    from HCRN model preprocessing\n",
    "    Args:\n",
    "        path: path of a video\n",
    "        num_clips: expected numbers of splitted clips\n",
    "        num_frames_per_clip: number of frames in a single clip, pretrained model only supports 16 frames\n",
    "    Returns:\n",
    "        A list of raw features of clips.\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "    clips = list()\n",
    "    try:\n",
    "        video_data = skvideo.io.vread(path)\n",
    "    except:\n",
    "        print('file {} error'.format(path))\n",
    "        valid = False\n",
    "        return list(np.zeros(shape=(num_clips, num_frames_per_clip, 3, 224, 224))), valid\n",
    "    \n",
    "    total_frames = video_data.shape[0]\n",
    "    img_size = (video_data[0].shape[1], video_data[0].shape[0]) # (width, height)\n",
    "    for i in np.linspace(0, total_frames, num_clips + 2, dtype=np.int32)[1:num_clips + 1]:\n",
    "        clip_start = int(i) - int(num_frames_per_clip / 2)\n",
    "        clip_end = int(i) + int(num_frames_per_clip / 2)\n",
    "        if clip_start < 0:\n",
    "            clip_start = 0\n",
    "        if clip_end > total_frames:\n",
    "            clip_end = total_frames - 1\n",
    "        clip = video_data[clip_start:clip_end]\n",
    "        if clip_start == 0:\n",
    "            shortage = num_frames_per_clip - (clip_end - clip_start)\n",
    "            added_frames = []\n",
    "            for _ in range(shortage):\n",
    "                added_frames.append(np.expand_dims(video_data[clip_start], axis=0))\n",
    "            if len(added_frames) > 0:\n",
    "                added_frames = np.concatenate(added_frames, axis=0)\n",
    "                clip = np.concatenate((added_frames, clip), axis=0)\n",
    "        if clip_end == (total_frames - 1):\n",
    "            shortage = num_frames_per_clip - (clip_end - clip_start)\n",
    "            added_frames = []\n",
    "            for _ in range(shortage):\n",
    "                added_frames.append(np.expand_dims(video_data[clip_end], axis=0))\n",
    "            if len(added_frames) > 0:\n",
    "                added_frames = np.concatenate(added_frames, axis=0)\n",
    "                clip = np.concatenate((clip, added_frames), axis=0)\n",
    "        new_clip = []\n",
    "        for j in range(num_frames_per_clip):\n",
    "            frame_data = clip[j]\n",
    "            img = Image.fromarray(frame_data)\n",
    "#             img = img.resize(img_size, resample=Image.BICUBIC)\n",
    "# #             img = img.transpose(2, 0, 1)\n",
    "#             frame_data = np.array(img)\n",
    "#             new_clip.append(frame_data)\n",
    "\n",
    "            #  DO CLIP encoding\n",
    "            image = preprocess(img).unsqueeze(0).to('cuda:0')\n",
    "            with torch.no_grad():\n",
    "                frame_feat = model.encode_image(image).squeeze().cpu().numpy()\n",
    "            new_clip.append(frame_feat)\n",
    "            \n",
    "#         new_clip = np.asarray(new_clip)  # (num_frames, width, height, channels)\n",
    "#         if args.model in ['resnext101']:\n",
    "#             new_clip = np.squeeze(new_clip)\n",
    "#             new_clip = np.transpose(new_clip, axes=(1, 0, 2, 3))\n",
    "        clips.append(new_clip)\n",
    "    return clips, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad2edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load('ViT-L/14@336px', device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae8f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid2path = load_json('../dataset/nextqa/map_vid_vidorID.json')\n",
    "\n",
    "mode = 'train'  #[train, val, test]\n",
    "csv_data = read_csv(f'../dataset/nextqa/{mode}.csv')\n",
    "h5_file = f'../../data/nextqa/frame_feat/clip_app_ft_{mode}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b143067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3238737531,  8968804598, 13884124143, ..., 10036075863,\n",
       "        7975580325, 10294585855])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data['video'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8392cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = []\n",
    "feats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4fd52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2574b6835b5430ba486e20f781376a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encoding video:   0%|          | 0/3870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=0\n",
    "for vid in tqdm(csv_data['video'].unique(), desc='encoding video', total=len(csv_data['video'].unique())):\n",
    "    if i <= 2274:\n",
    "        i+=1\n",
    "        continue\n",
    "    video_path = get_video_path(vid2path,vid)\n",
    "    raw_clip, vaild = extract_clips_with_consecutive_frames(video_path, 16, 4, preprocess, model)\n",
    "    if vaild:\n",
    "        vids.append(vid)\n",
    "        feats.append(raw_clip)\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "\n",
    "# vids = np.asarray(vids)\n",
    "# feats = np.asarray(feats)\n",
    "\n",
    "if not os.path.exists(h5_file):\n",
    "    f = h5py.File(h5_file, 'w')\n",
    "else:\n",
    "    f = h5py.File(h5_file, 'w')\n",
    "    \n",
    "f.create_dataset('ids', data=vids)\n",
    "f.create_dataset('features', data=feats)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a959f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check data\n",
    "fp = h5py.File(h5_file, 'r')\n",
    "load_vids = fp['ids']\n",
    "load_feats = fp['features']\n",
    "print(load_vids, np.array(load_feats).shape)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf0514a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = get_video_path(vid2path,2414793083)\n",
    "raw_clip, vaild = extract_clips_with_consecutive_frames(video_path, 16, 2, preprocess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44880ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(raw_clip).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b9d551",
   "metadata": {},
   "source": [
    "# CLIP test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9658c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ffb9019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list = clip.available_models()\n",
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8c1fdfd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "embed_list = {}\n",
    "for i in range(9):\n",
    "    model, preprocess = clip.load(model_list[i], device='cpu')\n",
    "    embed_list[model_list[i]] = model.encode_text(clip.tokenize('I you')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a934f05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RN50': torch.Size([1, 1024]),\n",
       " 'RN101': torch.Size([1, 512]),\n",
       " 'RN50x4': torch.Size([1, 640]),\n",
       " 'RN50x16': torch.Size([1, 768]),\n",
       " 'RN50x64': torch.Size([1, 1024]),\n",
       " 'ViT-B/32': torch.Size([1, 512]),\n",
       " 'ViT-B/16': torch.Size([1, 512]),\n",
       " 'ViT-L/14': torch.Size([1, 768]),\n",
       " 'ViT-L/14@336px': torch.Size([1, 768])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0c64e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = clip.load(model_list[-1], device='cpu')\n",
    "model.encode_text(clip.tokenize('I save your life')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4644660a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   328,  2673,   695,   970,   706,   269, 49407,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.tokenize('I save your life so.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc1ceb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5095e-01, -6.5090e-03,  5.4056e-01, -3.7528e-01, -6.8354e-02,\n",
       "          9.7832e-02, -3.5222e-01,  5.4487e-02,  2.1441e-01, -6.7708e-01,\n",
       "          1.7157e-01,  1.8295e-03, -8.2062e-01,  2.3109e-01, -4.7107e-01,\n",
       "          1.3864e-01,  4.9570e-02,  4.5823e-01, -2.9067e-01, -3.0055e-01,\n",
       "          6.6539e-02, -2.7940e-01,  8.0342e-02, -9.4442e-03,  9.9375e-02,\n",
       "         -5.1597e-01,  4.8310e-01, -8.9906e-02,  4.9464e-01,  1.8817e-01,\n",
       "         -3.9430e-01, -2.4976e-01, -1.4593e-01, -3.2245e-03, -3.8681e-01,\n",
       "          3.8197e-01,  2.2733e-01, -3.4141e-01, -4.3623e-01, -3.0771e-01,\n",
       "          5.8945e-02, -2.2727e-01,  1.0736e-01, -2.0524e-01,  2.6954e-02,\n",
       "         -2.8702e-01, -1.1606e-01, -1.4593e-02, -3.3899e-02, -5.6178e-01,\n",
       "          2.6186e-02, -1.1908e-01,  1.6539e-01, -1.8123e-01,  4.1063e-01,\n",
       "          3.2647e-02, -1.9460e-02, -1.7700e-02,  1.0868e-01,  2.7433e-01,\n",
       "         -3.3449e-01,  1.3509e-01,  1.7533e-01,  1.7811e-01, -2.4851e-01,\n",
       "          6.0435e-01,  4.0570e-01,  5.8146e-02,  1.1644e-01, -5.2316e-01,\n",
       "         -7.0033e-02,  6.9961e-01,  1.1809e-01, -1.0408e-01, -3.5027e-01,\n",
       "          3.1767e-01,  2.1875e-01,  1.0059e-01, -3.5365e-03, -8.1611e-01,\n",
       "         -2.5293e-01,  3.0611e-01,  7.0187e-02, -9.4572e-02,  2.4915e-01,\n",
       "          2.0976e-01, -1.7011e-01, -4.0409e-02, -4.2336e-01,  4.1401e-01,\n",
       "         -3.9134e-01,  2.6855e-01, -2.8086e-01,  3.2023e-01, -2.0940e-01,\n",
       "          8.3833e-03,  3.5148e-01, -1.5969e-01,  6.2378e-01, -4.9768e-01,\n",
       "         -1.9221e-02, -2.7044e-01, -3.1705e-01,  3.8919e-01,  1.3011e-01,\n",
       "          3.6702e-01,  5.0544e-01,  5.9849e-02,  4.5555e-01,  8.9459e-02,\n",
       "          1.7791e-01, -3.5562e-02, -7.1021e-02,  4.3874e-01,  3.6651e-02,\n",
       "          3.6401e-01, -6.7924e-01, -5.2673e-01,  2.9593e-02,  7.3339e-02,\n",
       "          2.4764e-02, -4.0676e-01,  3.8205e-02,  5.0881e-02, -2.6589e-01,\n",
       "         -7.2241e-02,  6.8215e-02,  3.5378e-01, -3.5281e-01, -3.2632e-01,\n",
       "          1.4818e-01, -8.3885e-01,  2.5059e-01, -5.6357e-01,  3.6658e-01,\n",
       "          4.3653e-01, -4.0307e-02,  3.9140e-01, -3.9529e-02, -2.0233e-01,\n",
       "          5.5104e-01, -6.9068e-02,  8.9554e-03, -3.6790e-01,  1.9011e-01,\n",
       "          1.3833e-01,  6.7024e-01, -7.1426e-01, -6.5580e-01,  5.6434e-02,\n",
       "          5.2895e-01, -2.9523e-01,  4.5334e-01, -1.8173e-01,  7.9016e-01,\n",
       "         -3.9942e-02,  1.9318e-01,  2.9075e-01,  1.2844e-01, -5.7622e-02,\n",
       "          6.8146e-01,  1.9424e-01,  1.5757e-01,  1.1833e-01, -1.8584e-01,\n",
       "         -4.7045e-01,  5.7846e-01, -2.7273e-01,  1.8919e-01, -6.0377e-01,\n",
       "          8.2707e-02, -3.7918e-01,  3.5538e-01, -3.8393e-02,  1.6319e-01,\n",
       "         -4.0046e-01,  1.1170e-01, -1.6727e+00,  2.8640e-01,  5.8093e-01,\n",
       "          1.1262e+00, -4.2970e-02, -3.3875e-01,  4.8238e-01, -2.1561e-01,\n",
       "         -3.6518e-01, -2.0253e-01,  4.3552e-01, -2.7585e-01, -3.6394e-01,\n",
       "         -3.8291e-01, -2.6522e-01,  1.1561e-01,  1.3277e-01,  8.8446e-02,\n",
       "         -2.2792e+00, -1.1964e-01,  9.8970e-02, -1.9751e-01,  4.0457e-01,\n",
       "          1.7345e-01, -2.0756e-01,  1.1161e-01, -1.0092e-01,  1.4037e-01,\n",
       "         -3.1165e-01, -4.9799e-01,  1.6341e-01,  6.1927e-01, -2.3065e-01,\n",
       "         -2.0818e-01,  8.2939e-02, -3.2608e-01,  3.8994e-01,  6.6566e-01,\n",
       "          7.5396e-02,  1.2098e-01,  3.0498e-01,  2.8331e-01,  4.0101e-01,\n",
       "          9.9210e-02, -1.8534e-01,  4.3960e-01, -2.1792e-01,  9.2985e-04,\n",
       "          1.8254e-01,  1.0173e-01, -3.0412e-02,  3.9452e-01, -1.3845e-01,\n",
       "         -7.3584e-02, -1.3009e-02, -3.5334e-02, -5.0193e-01,  4.5040e-01,\n",
       "         -1.1949e-02, -2.9764e-01,  6.8414e-02,  5.2059e-01, -5.6709e-01,\n",
       "          7.3535e-02,  1.2955e-01,  1.4748e-01,  2.2023e-01, -1.5031e-01,\n",
       "          5.6592e-01,  2.9548e-01,  1.7031e-01,  3.0257e-01,  5.4202e-01,\n",
       "          3.0087e-01,  3.6886e-01, -2.7971e-01, -3.9295e-01,  4.8727e-01,\n",
       "          4.5225e-01, -1.7037e-01,  7.1512e-01,  7.1066e-02,  8.0691e-01,\n",
       "         -3.4655e-01,  1.6143e-02,  9.4958e-03,  1.3426e-01,  4.2884e-01,\n",
       "          3.6915e-01, -9.5577e-02, -5.9707e-01, -3.3016e-01, -4.1439e-01,\n",
       "         -2.6130e-01, -2.5354e-01,  3.6662e-01, -5.6967e-01, -3.8617e-02,\n",
       "         -2.2351e-01,  5.8625e-02,  1.9318e-01,  3.2052e-02, -9.2659e-02,\n",
       "         -1.2743e-01, -9.1460e-01, -2.8429e-01,  1.0965e-01,  1.7806e-01,\n",
       "         -1.7362e-01,  1.0992e-01,  2.1360e-01, -2.0780e+00, -1.2193e-01,\n",
       "         -4.4958e-01, -4.3308e-02, -8.2599e-01, -1.6121e-01, -4.8727e-01,\n",
       "          3.8638e-01, -5.7632e-01,  3.0201e-01,  1.4036e-01,  1.2760e-01,\n",
       "         -3.5921e-01, -2.5040e-01, -1.8595e-01, -1.7317e-01,  2.2774e-01,\n",
       "         -8.8528e-02, -2.7738e-01, -8.2149e-02,  1.9224e-02, -2.0847e-01,\n",
       "         -6.5395e-01,  3.4184e-02,  2.3138e-02, -3.5992e-01,  2.1028e-01,\n",
       "         -2.3202e-01, -1.2532e+01,  2.4967e-04, -8.0978e-01,  2.1112e-01,\n",
       "         -1.0314e-01, -5.1403e-01, -2.2725e-02,  4.8550e-01, -2.7802e-01,\n",
       "          6.4078e-01, -9.3209e-01, -1.3387e-01,  5.6316e-02, -9.5378e-02,\n",
       "         -1.0090e-01,  4.1623e-01, -8.4130e-02, -1.6776e-01,  1.5622e-02,\n",
       "          1.3077e-01, -2.3767e-01, -2.1431e-01, -5.9638e-01, -2.5359e-01,\n",
       "         -4.4586e-01, -1.0056e-01, -6.5422e-02,  1.3843e-01,  2.2862e-01,\n",
       "         -1.2952e-01,  3.9216e-01,  6.0902e-03,  4.9809e-01,  8.0658e-02,\n",
       "          8.3828e-02,  1.0226e-01, -2.1832e-01,  4.6759e-02,  2.1799e-01,\n",
       "         -3.9136e-01,  7.3316e-01,  1.1291e-01, -4.8325e-01, -2.9825e-01,\n",
       "          2.1615e-01,  2.0261e-01,  7.3425e-01,  4.1737e-01,  5.4302e-01,\n",
       "          5.1354e-01,  4.9736e-01,  1.3207e-01, -3.5074e-03,  5.3806e-01,\n",
       "         -1.2721e-01,  3.5221e-02, -4.2023e-03, -8.6540e-02, -1.8078e-01,\n",
       "         -4.0508e-02, -2.4526e-01,  3.3243e-03,  2.7886e-01, -5.6413e-02,\n",
       "          2.9448e-01, -7.4320e-01, -3.3131e-01, -1.5849e-01,  2.5625e-01,\n",
       "         -1.9249e-01,  4.5979e-01, -4.2120e-02,  3.9080e-02, -3.9098e-02,\n",
       "          4.9026e-01, -1.0487e-01, -2.5155e-01, -2.2478e-01, -3.4362e-01,\n",
       "         -1.2067e-01, -1.4498e-01,  8.6651e-02, -1.2130e-01,  2.8544e-01,\n",
       "          3.7098e-01,  3.1521e-01,  1.1137e-01, -3.7758e-02,  9.9213e-01,\n",
       "          6.5611e-01, -3.3286e-01,  1.2709e-01,  3.6274e-02,  5.0320e-01,\n",
       "         -2.5765e-02, -5.0820e-01,  4.5659e-01, -1.3293e+00,  2.6765e-01,\n",
       "          3.8246e-01, -5.0543e-01, -5.2573e-01, -1.5254e-01, -4.0634e-01,\n",
       "         -5.4618e-01,  2.1696e-01,  3.9395e-01, -4.6455e-01, -1.1441e-01,\n",
       "         -1.6626e-01, -2.7555e-01,  3.3143e-01,  6.1285e-02, -2.4000e-01,\n",
       "         -5.3666e-01,  2.0528e-01,  6.8959e-01, -4.2044e-01,  1.1842e-01,\n",
       "          9.5804e-01,  1.3147e-01,  1.1079e-01, -4.6073e-01, -8.1695e-02,\n",
       "          1.1368e+01,  4.6676e-02,  8.0429e-01,  4.5934e-01,  4.0448e-03,\n",
       "         -3.5346e-01,  2.7774e-01,  2.9658e-01,  4.1458e-01, -6.0518e-01,\n",
       "         -1.2282e+00, -5.1762e-01,  4.4312e-01,  2.1621e-02, -1.1881e-01,\n",
       "          5.5176e-01,  2.5604e-01,  1.1730e-01,  4.1032e-02,  2.5138e-01,\n",
       "         -9.2264e-01,  3.4530e-01,  2.9634e-01,  4.9854e-01, -6.8533e-02,\n",
       "         -1.6883e-01,  2.5170e-01,  6.0847e-01, -8.5792e-02,  2.1117e-01,\n",
       "         -9.0164e-02, -3.1019e-01,  3.4334e-01, -6.6038e-01,  4.0423e-01,\n",
       "         -4.3994e-01,  3.6104e-01,  2.5592e-01, -2.0131e-01, -1.0572e-01,\n",
       "         -8.9334e-02, -5.0504e-01, -6.8819e-02, -3.9313e-01, -5.1095e-01,\n",
       "         -8.4190e-02,  3.6411e-01, -4.0419e-01,  8.5523e-02,  3.1418e-01,\n",
       "          1.1646e-01, -4.9808e-02, -5.8424e-01,  3.2427e-01, -2.7879e-01,\n",
       "         -2.0108e-01,  1.8954e-01, -5.3455e-01, -2.5156e-02,  1.6171e-01,\n",
       "          1.4154e-01, -3.2007e-01,  1.0236e-01,  6.4827e-02, -4.6885e-01,\n",
       "         -1.7920e-01, -2.0718e-01, -2.3817e-03, -8.2834e-01,  7.6152e-02,\n",
       "          2.6697e-01, -4.7602e-01,  2.1169e-01, -1.2843e-02, -4.0424e-01,\n",
       "         -1.3274e-01,  1.2938e-01, -7.2329e-02,  1.7716e-02, -3.6189e-01,\n",
       "          8.9147e-02, -3.3671e-01,  4.5753e-02, -3.0608e-01, -4.9028e-02,\n",
       "          5.2713e-01,  5.3817e-01,  1.2894e-01, -1.8927e+00, -1.4063e-01,\n",
       "         -6.4493e-01, -5.3381e-01, -3.6839e-01,  2.2918e-01,  3.5812e-01,\n",
       "         -1.3478e-01, -4.8352e-02, -3.8580e-01, -7.6041e-01, -2.1583e-01,\n",
       "         -2.3345e-01,  2.5554e-01, -1.3915e-01, -5.4954e-02, -1.0845e-01,\n",
       "          6.8307e-02,  2.3371e-01, -2.2661e-01,  2.7474e-01, -7.0500e-02,\n",
       "          2.6172e-01, -3.1370e-01,  2.5036e-01,  3.2762e-01, -4.1254e-02,\n",
       "          4.4333e-01, -2.9851e-02,  6.2835e-01, -2.7805e-01,  5.3772e-02,\n",
       "          7.8497e-01,  4.2123e-01, -2.8844e-01,  9.2309e-02,  9.7192e-01,\n",
       "          2.1185e-01,  3.3005e-01,  1.1243e-02,  1.9204e-01,  5.5382e-01,\n",
       "          2.5197e-01, -4.1893e-01, -3.8730e-01, -4.4712e-01,  3.0405e-01,\n",
       "         -1.9707e-01,  6.6090e-02, -8.9520e-02, -4.2868e-02, -2.7068e-01,\n",
       "          2.7105e-01, -1.9613e-02, -2.7082e-01,  2.6466e-01, -3.8699e-01,\n",
       "         -9.7142e-01,  5.5105e-01, -1.4743e-01,  7.4628e-02, -3.9573e-01,\n",
       "          7.2629e-02,  3.7049e-02, -6.1190e-01, -2.4363e-01,  5.8442e-02,\n",
       "          2.0862e-01,  2.8501e-01, -3.7159e-01,  5.5089e-02,  5.6621e-01,\n",
       "          8.5496e-02,  5.1626e-01,  8.8702e-02,  2.3722e-01, -1.4572e-01,\n",
       "          3.5203e-01,  3.3600e-01,  4.6525e-02, -1.4878e-02,  1.5067e-01,\n",
       "          2.8157e-01, -3.0407e-01, -6.2172e-01, -7.6077e-02,  1.5346e-01,\n",
       "         -2.2570e-01,  2.6366e-01, -1.1673e+00,  5.8822e-01, -2.4291e-02,\n",
       "          3.0412e-01,  2.3821e-01, -2.2453e-01,  3.7548e-01, -5.0362e-01,\n",
       "          6.1300e-01,  2.3451e-01, -1.9117e-01,  1.4995e-01, -3.7339e-01,\n",
       "         -2.1085e-01,  3.6730e-01, -1.6019e-01, -8.2923e-02,  3.8449e-02,\n",
       "         -4.7249e-02,  4.3278e-01, -7.0069e-01,  2.9863e-01, -2.4998e-01,\n",
       "          7.8877e-03,  5.3648e-01, -9.7941e-02, -4.5082e-01,  1.3364e+00,\n",
       "          1.2299e-01,  5.5104e-02, -9.8776e-02,  5.2727e-01, -4.0966e-01,\n",
       "          7.8082e-02, -2.6237e-01, -2.6219e-01, -1.2151e-01,  2.8677e-01,\n",
       "          2.8948e-01, -6.0716e-02, -3.9504e-03, -1.8712e-02, -5.0975e-01,\n",
       "         -5.8249e-01, -3.6684e-01,  3.8226e-02,  4.6624e-01,  2.0258e-01,\n",
       "         -3.3925e-02,  1.8737e-02,  1.4165e-01, -1.0424e+00,  1.4860e-01,\n",
       "         -2.3785e-01,  9.5380e-02, -4.5170e-02,  4.9374e-01,  1.6325e-01,\n",
       "         -2.8255e-01,  1.1667e-01, -1.0670e-01, -2.4789e-01, -1.9301e-01,\n",
       "         -1.3108e-01, -1.2544e-02, -2.6140e-01, -5.3350e-01,  4.4764e-01,\n",
       "          3.0115e-01, -1.4447e-01,  2.4566e-01,  3.8044e-01, -2.7379e-02,\n",
       "          2.5735e-01,  2.5025e-01,  6.8675e-03, -2.1934e-01, -3.9600e-02,\n",
       "          2.6542e-02, -7.5627e-03, -5.4456e-02, -2.8021e-01, -3.0087e-01,\n",
       "          1.1835e-01, -1.5522e-01, -1.2500e-02, -1.0037e+00,  2.0985e-01,\n",
       "          3.6325e-01, -2.0265e-01, -1.4178e-01, -7.0879e-03, -2.3804e-01,\n",
       "         -2.4749e-01,  1.8701e-01, -3.9367e-01,  5.3138e-01,  2.1797e-01,\n",
       "          5.9845e-02,  2.5671e-01, -3.1943e-02,  3.3261e-01, -4.1760e-02,\n",
       "          3.5626e-01,  1.3537e-01,  2.1715e-01, -7.3842e-01,  9.6648e-01,\n",
       "          1.8472e-01,  1.9243e-01, -5.6156e-02, -4.6576e-01,  2.7961e-01,\n",
       "          6.0792e-02,  2.4229e-02,  1.4969e-01, -5.6457e-01,  6.0541e-01,\n",
       "          3.6936e-01,  3.1523e-01,  1.2888e-01,  5.3425e-01,  6.3764e-01,\n",
       "          3.8537e-01,  2.9048e-01, -1.7910e-01,  4.1827e-01,  2.2587e-01,\n",
       "          1.5832e-01,  1.5970e-01,  3.0336e-01,  4.0183e-01, -4.5123e-01,\n",
       "         -5.1527e-03, -1.1739e-01,  6.2437e-02, -9.2011e-02, -3.1400e-01,\n",
       "         -5.1356e-01,  5.6094e-01,  3.7494e-01, -1.5158e-01, -2.2885e-01,\n",
       "          4.9155e-02, -5.8592e-01,  9.2207e-01,  5.7491e-01,  5.1799e-01,\n",
       "         -5.4464e-01, -4.9856e-01, -2.8436e-02]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode_text(clip.tokenize('I'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebd985e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1510, -0.0065,  0.5406,  ..., -0.5446, -0.4986, -0.0284],\n",
       "        [ 0.1440, -0.0338,  0.3838,  ..., -0.2692, -0.1916, -0.2707]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([model.encode_text(clip.tokenize('I')),\n",
    "          model.encode_text(clip.tokenize('not'))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
